{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"tf.__version__","metadata":{"execution":{"iopub.status.busy":"2022-04-12T03:13:50.664707Z","iopub.execute_input":"2022-04-12T03:13:50.665088Z","iopub.status.idle":"2022-04-12T03:13:50.673132Z","shell.execute_reply.started":"2022-04-12T03:13:50.665051Z","shell.execute_reply":"2022-04-12T03:13:50.671723Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport matplotlib.pyplot as plt\nimport collections\nimport random\nimport numpy as np\nfrom tensorflow.keras.initializers import Constant\nimport pandas as pd\nimport os\nfrom tensorflow import keras\nimport time\nimport re\nfrom tqdm import tqdm\nimport json\nfrom PIL import Image\nfrom tensorflow.keras.layers.experimental.preprocessing import Resizing\nimage_path = '../input/flickr8k/Images/'\ndf = pd.read_csv('../input/flickr8k/captions.txt')","metadata":{"execution":{"iopub.status.busy":"2022-04-12T03:13:46.496600Z","iopub.execute_input":"2022-04-12T03:13:46.496946Z","iopub.status.idle":"2022-04-12T03:13:46.572072Z","shell.execute_reply.started":"2022-04-12T03:13:46.496911Z","shell.execute_reply":"2022-04-12T03:13:46.571094Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"def auto_select_accelerator():\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n        print(\"Running on TPU:\", tpu.master())\n    except ValueError:\n        print(\"Not running on TPUs\")\n        strategy = tf.distribute.get_strategy()\n    print(f\"Running on {strategy.num_replicas_in_sync} replicas\")\n    \n    return strategy\n\nstrategy = auto_select_accelerator()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T03:13:54.467755Z","iopub.execute_input":"2022-04-12T03:13:54.468254Z","iopub.status.idle":"2022-04-12T03:14:00.883724Z","shell.execute_reply.started":"2022-04-12T03:13:54.468219Z","shell.execute_reply":"2022-04-12T03:14:00.882367Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"def process_caption(text):\n    text = text.lower()\n    text = re.sub(\"[^A-Za-z]\",\" \", str(text))\n    text = text[:-2]\n    return text\ndf['caption'] = df['caption'].apply(lambda x: process_caption(x))","metadata":{"execution":{"iopub.status.busy":"2022-04-12T03:14:03.177521Z","iopub.execute_input":"2022-04-12T03:14:03.177821Z","iopub.status.idle":"2022-04-12T03:14:03.547026Z","shell.execute_reply.started":"2022-04-12T03:14:03.177792Z","shell.execute_reply":"2022-04-12T03:14:03.545661Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"image_path = '../input/flickr8k/Images/'\nimage_path_to_caption = collections.defaultdict(list)\n\ncaptions = df['caption']\ntrain_captions = []\nimg_name_vector = []\n\nfor index,row in tqdm(df.iterrows()):\n    caption = f\"<start> {row['caption']} <end>\"\n    image_path_new  = image_path + row['image']\n    image_path_to_caption[image_path_new].append(caption)\nprint(len(image_path_to_caption.keys()))","metadata":{"execution":{"iopub.status.busy":"2022-04-12T03:14:07.121631Z","iopub.execute_input":"2022-04-12T03:14:07.121961Z","iopub.status.idle":"2022-04-12T03:14:09.729664Z","shell.execute_reply.started":"2022-04-12T03:14:07.121928Z","shell.execute_reply":"2022-04-12T03:14:09.728592Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"##change this to full dataset when training\ntrain_image_paths = list(image_path_to_caption.keys())\ntrain_image_paths = train_image_paths[:]\n\nrandom.shuffle(train_image_paths)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T03:14:11.188333Z","iopub.execute_input":"2022-04-12T03:14:11.188620Z","iopub.status.idle":"2022-04-12T03:14:11.203610Z","shell.execute_reply.started":"2022-04-12T03:14:11.188592Z","shell.execute_reply":"2022-04-12T03:14:11.202648Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"train_captions = []\nimg_name_vector = []\n\nfor path in train_image_paths:\n      caption_list = image_path_to_caption[path]\n      train_captions.extend(caption_list)\n      img_name_vector.extend([path] * len(caption_list))","metadata":{"execution":{"iopub.status.busy":"2022-04-12T03:14:15.575253Z","iopub.execute_input":"2022-04-12T03:14:15.575522Z","iopub.status.idle":"2022-04-12T03:14:15.594987Z","shell.execute_reply.started":"2022-04-12T03:14:15.575495Z","shell.execute_reply":"2022-04-12T03:14:15.593725Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"def load_image(image_path):\n    img = tf.io.read_file(image_path)\n    img = tf.io.decode_jpeg(img, channels=3)\n    img = Resizing(299, 299)(img)\n    img = tf.keras.applications.inception_v3.preprocess_input(img)\n    return img, image_path","metadata":{"execution":{"iopub.status.busy":"2022-04-12T03:14:16.920464Z","iopub.execute_input":"2022-04-12T03:14:16.920761Z","iopub.status.idle":"2022-04-12T03:14:16.926564Z","shell.execute_reply.started":"2022-04-12T03:14:16.920732Z","shell.execute_reply":"2022-04-12T03:14:16.925796Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"image_model = tf.keras.applications.InceptionV3(include_top=False,weights='imagenet')\nnew_input = image_model.input\nhidden_layer = image_model.layers[-1].output\nimage_features_extract_model = tf.keras.Model(new_input, hidden_layer)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T03:14:17.555982Z","iopub.execute_input":"2022-04-12T03:14:17.556357Z","iopub.status.idle":"2022-04-12T03:14:21.249370Z","shell.execute_reply.started":"2022-04-12T03:14:17.556321Z","shell.execute_reply":"2022-04-12T03:14:21.248177Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"encode_train = sorted(set(img_name_vector))\n\nimage_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\nimage_dataset = image_dataset.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(32)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T03:14:22.585311Z","iopub.execute_input":"2022-04-12T03:14:22.586175Z","iopub.status.idle":"2022-04-12T03:14:22.663934Z","shell.execute_reply.started":"2022-04-12T03:14:22.586127Z","shell.execute_reply":"2022-04-12T03:14:22.662589Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"for img, path in image_dataset:\n    print(img,path)\n    batch_features = image_features_extract_model(img)\n    batch_features = tf.reshape(batch_features,\n                              (batch_features.shape[0], -1, batch_features.shape[3]))\n    \n    for bf, p in zip(batch_features, path):\n        path_of_feature = p.numpy().decode(\"utf-8\")\n        path_of_feature = path_of_feature[25:]\n        np.save(path_of_feature, bf.numpy())","metadata":{"execution":{"iopub.status.busy":"2022-04-12T03:14:39.897086Z","iopub.execute_input":"2022-04-12T03:14:39.897458Z","iopub.status.idle":"2022-04-12T03:14:39.903398Z","shell.execute_reply.started":"2022-04-12T03:14:39.897426Z","shell.execute_reply":"2022-04-12T03:14:39.901861Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"caption_dataset = tf.data.Dataset.from_tensor_slices(train_captions)\n\ndef standardize(inputs):\n  inputs = tf.strings.lower(inputs)\n  return tf.strings.regex_replace(inputs,\n                                  r\"!\\\"#$%&\\(\\)\\*\\+.,-/:;=?@\\[\\\\\\]^_`{|}~\", \"\")\n\nmax_length = 50\nvocabulary_size = 5000\ntokenizer = tf.keras.layers.experimental.preprocessing.TextVectorization(max_tokens =vocabulary_size,\n                                                                         standardize=standardize,\n                                                                        output_sequence_length=50)\ntokenizer.adapt(caption_dataset)\n\ndef vectorize_text(text):\n    text = tf.expand_dims(text, -1)\n    return tokenizer(text)\n\ncap_vector = caption_dataset.map(vectorize_text)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T03:14:43.104401Z","iopub.execute_input":"2022-04-12T03:14:43.104723Z","iopub.status.idle":"2022-04-12T03:17:53.674632Z","shell.execute_reply.started":"2022-04-12T03:14:43.104693Z","shell.execute_reply":"2022-04-12T03:17:53.673403Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"#loading glove\nembedding_dict={}\nwith open('../input/glove-embeddings/glove.6B.200d.txt','r') as f:\n    for line in f:\n        values=line.split()\n        word = values[0]\n        vectors=np.asarray(values[1:],'float32')\n        embedding_dict[word]=vectors\nf.close()","metadata":{"execution":{"iopub.status.busy":"2022-04-12T03:17:53.678038Z","iopub.execute_input":"2022-04-12T03:17:53.678364Z","iopub.status.idle":"2022-04-12T03:18:29.854800Z","shell.execute_reply.started":"2022-04-12T03:17:53.678329Z","shell.execute_reply":"2022-04-12T03:18:29.853765Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"vocab = len(tokenizer.get_vocabulary()) \nembedding_matrix = np.zeros((vocab,200))\n            \nfor token, word in tqdm(enumerate(tokenizer.get_vocabulary())):\n    if(word in embedding_dict.keys()):\n        embedding_matrix[token] = embedding_dict[word]","metadata":{"execution":{"iopub.status.busy":"2022-04-12T03:05:08.873827Z","iopub.execute_input":"2022-04-12T03:05:08.874081Z","iopub.status.idle":"2022-04-12T03:05:08.905436Z","shell.execute_reply.started":"2022-04-12T03:05:08.87405Z","shell.execute_reply":"2022-04-12T03:05:08.903709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_to_index = tf.keras.layers.experimental.preprocessing.StringLookup(\n    mask_token=\"\",\n    vocabulary=tokenizer.get_vocabulary())\n\nindex_to_word = tf.keras.layers.experimental.preprocessing.StringLookup(\n    mask_token=\"\",\n    vocabulary=tokenizer.get_vocabulary(),\n    invert=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T03:05:08.906415Z","iopub.status.idle":"2022-04-12T03:05:08.907773Z","shell.execute_reply.started":"2022-04-12T03:05:08.907546Z","shell.execute_reply":"2022-04-12T03:05:08.907573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_to_cap_vector = collections.defaultdict(list)\nfor img, cap in zip(img_name_vector, cap_vector):\n    cap = tf.squeeze(cap)\n    img_to_cap_vector[img].append(cap)\n\nimg_keys = list(img_to_cap_vector.keys())\nrandom.shuffle(img_keys)\n\nslice_index = int(len(img_keys)*0.95)\nimg_name_train_keys, img_name_val_keys = img_keys[:slice_index], img_keys[slice_index:]\n\nimg_name_train = []\ncap_train = []\nfor imgt in img_name_train_keys:\n    capt_len = len(img_to_cap_vector[imgt])\n    img_name_train.extend([imgt] * capt_len)\n    cap_train.extend(img_to_cap_vector[imgt])\n\nimg_name_val = []\ncap_val = []\n\nfor imgv in img_name_val_keys:\n    capv_len = len(img_to_cap_vector[imgv])\n    img_name_val.extend([imgv] * capv_len)\n    cap_val.extend(img_to_cap_vector[imgv])\n","metadata":{"execution":{"iopub.status.busy":"2022-04-12T03:05:08.909157Z","iopub.status.idle":"2022-04-12T03:05:08.90949Z","shell.execute_reply.started":"2022-04-12T03:05:08.909315Z","shell.execute_reply":"2022-04-12T03:05:08.909337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(img_name_train), len(cap_train), len(img_name_val), len(cap_val)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T03:05:56.41941Z","iopub.execute_input":"2022-04-12T03:05:56.419705Z","iopub.status.idle":"2022-04-12T03:05:56.446111Z","shell.execute_reply.started":"2022-04-12T03:05:56.419668Z","shell.execute_reply":"2022-04-12T03:05:56.445188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 64\nBUFFER_SIZE = 1000\nembedding_dim = 200\nunits = 512\nnum_steps = len(img_name_train) // BATCH_SIZE\n\n# Shape of the vector extracted from InceptionV3 is (64, 2048)\n# These two variables represent that vector shape\nfeatures_shape = 2048\nattention_features_shape = 64","metadata":{"execution":{"iopub.status.busy":"2022-04-12T03:05:08.912299Z","iopub.status.idle":"2022-04-12T03:05:08.91267Z","shell.execute_reply.started":"2022-04-12T03:05:08.912493Z","shell.execute_reply":"2022-04-12T03:05:08.912515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def map_func(img_name, cap):\n  img_name = img_name.decode('utf-8')[25:]\n  img_tensor = np.load('./'+ img_name  +'.npy')\n  return img_tensor, cap","metadata":{"execution":{"iopub.status.busy":"2022-04-12T03:05:08.913651Z","iopub.status.idle":"2022-04-12T03:05:08.914789Z","shell.execute_reply.started":"2022-04-12T03:05:08.914573Z","shell.execute_reply":"2022-04-12T03:05:08.914592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\n\ndataset = dataset.map(lambda item1, item2: tf.numpy_function(\n          map_func, [item1, item2], [tf.float32, tf.int64]),\n          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n\ndataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\ndataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2022-04-12T03:05:08.915721Z","iopub.status.idle":"2022-04-12T03:05:08.916376Z","shell.execute_reply.started":"2022-04-12T03:05:08.916092Z","shell.execute_reply":"2022-04-12T03:05:08.916149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BahdanauAttention(tf.keras.Model):\n  def __init__(self, units):\n    super(BahdanauAttention, self).__init__()\n    self.W1 = tf.keras.layers.Dense(units)\n    self.W2 = tf.keras.layers.Dense(units)\n    self.V = tf.keras.layers.Dense(1)\n\n  def call(self, features, hidden):\n    # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n\n    # hidden shape == (batch_size, hidden_size)\n    # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n    hidden_with_time_axis = tf.expand_dims(hidden, 1)\n\n    # attention_hidden_layer shape == (batch_size, 64, units)\n    attention_hidden_layer = (tf.nn.tanh(self.W1(features) +\n                                         self.W2(hidden_with_time_axis)))\n\n    # score shape == (batch_size, 64, 1)\n    # This gives you an unnormalized score for each image feature.\n    score = self.V(attention_hidden_layer)\n\n    # attention_weights shape == (batch_size, 64, 1)\n    attention_weights = tf.nn.softmax(score, axis=1)\n\n    # context_vector shape after sum == (batch_size, hidden_size)\n    context_vector = attention_weights * features\n    context_vector = tf.reduce_sum(context_vector, axis=1)\n\n    return context_vector, attention_weights\n","metadata":{"execution":{"iopub.status.busy":"2022-04-12T03:05:08.917716Z","iopub.status.idle":"2022-04-12T03:05:08.918545Z","shell.execute_reply.started":"2022-04-12T03:05:08.918361Z","shell.execute_reply":"2022-04-12T03:05:08.918381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CNN_Encoder(tf.keras.Model):\n    # Since you have already extracted the features and dumped it\n    # This encoder passes those features through a Fully connected layer\n    def __init__(self, embedding_dim):\n        super(CNN_Encoder, self).__init__()\n        # shape after fc == (batch_size, 64, embedding_dim)\n        self.fc = tf.keras.layers.Dense(embedding_dim)\n\n    def call(self, x):\n        x = self.fc(x)\n        x = tf.nn.relu(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-04-12T03:05:08.919511Z","iopub.status.idle":"2022-04-12T03:05:08.920284Z","shell.execute_reply.started":"2022-04-12T03:05:08.920056Z","shell.execute_reply":"2022-04-12T03:05:08.920078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RNN_Decoder(tf.keras.Model):\n  def __init__(self, embedding_dim, units, vocab_size):\n    super(RNN_Decoder, self).__init__()\n    self.units = units\n\n    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim, embeddings_initializer=Constant(embedding_matrix),\n                                               trainable=False)\n    self.gru = tf.keras.layers.GRU(self.units,\n                                   return_sequences=True,\n                                   return_state=True,\n                                   recurrent_initializer='glorot_uniform')\n    self.fc1 = tf.keras.layers.Dense(self.units)\n    self.fc2 = tf.keras.layers.Dense(vocab_size)\n\n    self.attention = BahdanauAttention(self.units)\n\n  def call(self, x, features, hidden):\n    # defining attention as a separate model\n    context_vector, attention_weights = self.attention(features, hidden)\n\n    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n    x = self.embedding(x)\n\n    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n\n    # passing the concatenated vector to the GRU\n    output, state = self.gru(x)\n\n    # shape == (batch_size, max_length, hidden_size)\n    x = self.fc1(output)\n\n    # x shape == (batch_size * max_length, hidden_size)\n    x = tf.reshape(x, (-1, x.shape[2]))\n\n    # output shape == (batch_size * max_length, vocab)\n    x = self.fc2(x)\n\n    return x, state, attention_weights\n\n  def reset_state(self, batch_size):\n    return tf.zeros((batch_size, self.units))\n","metadata":{"execution":{"iopub.status.busy":"2022-04-12T03:05:08.921182Z","iopub.status.idle":"2022-04-12T03:05:08.921948Z","shell.execute_reply.started":"2022-04-12T03:05:08.921747Z","shell.execute_reply":"2022-04-12T03:05:08.921768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    encoder = CNN_Encoder(embedding_dim)\n    decoder = RNN_Decoder(embedding_dim, units, len(tokenizer.get_vocabulary()))\n    optimizer = tf.keras.optimizers.Adam() \n    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n    from_logits=True, reduction='none')","metadata":{"execution":{"iopub.status.busy":"2022-04-12T03:05:08.922776Z","iopub.status.idle":"2022-04-12T03:05:08.923573Z","shell.execute_reply.started":"2022-04-12T03:05:08.923316Z","shell.execute_reply":"2022-04-12T03:05:08.923344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def loss_function(real, pred):\n  mask = tf.math.logical_not(tf.math.equal(real, 0))\n  loss_ = loss_object(real, pred)\n\n  mask = tf.cast(mask, dtype=loss_.dtype)\n  loss_ *= mask\n\n  return tf.reduce_mean(loss_)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-12T03:05:08.924554Z","iopub.status.idle":"2022-04-12T03:05:08.92521Z","shell.execute_reply.started":"2022-04-12T03:05:08.92498Z","shell.execute_reply":"2022-04-12T03:05:08.924999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint_path = \"./checkpoints/train\"\nckpt = tf.train.Checkpoint(encoder=encoder,\n                           decoder=decoder,\n                           optimizer=optimizer)\nckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-12T03:05:08.926304Z","iopub.status.idle":"2022-04-12T03:05:08.926973Z","shell.execute_reply.started":"2022-04-12T03:05:08.926772Z","shell.execute_reply":"2022-04-12T03:05:08.926793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_epoch = 0\nif ckpt_manager.latest_checkpoint:\n  start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n  # restoring the latest checkpoint in checkpoint_path\n  ckpt.restore(ckpt_manager.latest_checkpoint)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-12T03:05:08.927896Z","iopub.status.idle":"2022-04-12T03:05:08.928531Z","shell.execute_reply.started":"2022-04-12T03:05:08.928332Z","shell.execute_reply":"2022-04-12T03:05:08.928353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_plot = []\nTrain = True","metadata":{"execution":{"iopub.status.busy":"2022-04-12T03:05:08.929486Z","iopub.status.idle":"2022-04-12T03:05:08.930149Z","shell.execute_reply.started":"2022-04-12T03:05:08.929944Z","shell.execute_reply":"2022-04-12T03:05:08.929962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@tf.function\ndef train_step(img_tensor, target):\n  loss = 0\n\n  # initializing the hidden state for each batch\n  # because the captions are not related from image to image\n  hidden = decoder.reset_state(batch_size=target.shape[0])\n\n  dec_input = tf.expand_dims([word_to_index('<start>')] * target.shape[0], 1)\n\n  with tf.GradientTape() as tape:\n      features = encoder(img_tensor)\n      #print(features.shape)\n      #print(target)\n      for i in range(1, target.shape[1]):\n          # passing the features through the decoder\n          predictions, hidden, _ = decoder(dec_input, features, hidden)\n          #print(predictions,hidden)\n\n          loss += loss_function(target[:, i], predictions)\n\n          # using teacher forcing\n          dec_input = tf.expand_dims(target[:, i], 1)\n\n  total_loss = (loss / int(target.shape[1]))\n\n  trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n\n  gradients = tape.gradient(loss, trainable_variables)\n\n  optimizer.apply_gradients(zip(gradients, trainable_variables))\n\n  return loss, total_loss\n","metadata":{"execution":{"iopub.status.busy":"2022-04-12T03:05:48.561475Z","iopub.execute_input":"2022-04-12T03:05:48.5618Z","iopub.status.idle":"2022-04-12T03:05:48.57281Z","shell.execute_reply.started":"2022-04-12T03:05:48.561765Z","shell.execute_reply":"2022-04-12T03:05:48.572155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EPOCHS = 15\nif Train:\n    \n    for epoch in tqdm(range(start_epoch, EPOCHS)):\n        start = time.time()\n        total_loss = 0\n\n        for (batch, (img_tensor, target)) in enumerate(dataset):\n            \n            batch_loss, t_loss = train_step(img_tensor, target)\n            total_loss += t_loss\n\n            if batch % 100 == 0:\n                average_batch_loss = batch_loss.numpy()/int(target.shape[1])\n                print(f'Epoch {epoch+1} Batch {batch} Loss {average_batch_loss:.4f}')\n        # storing the \n        #epoch end loss value to plot later\n        loss_plot.append(total_loss / num_steps)\n\n#         if epoch % 5 == 0:\n#           ckpt_manager.save()\n\n        print(f'Epoch {epoch+1} Loss {total_loss/num_steps:.6f}')\n        print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')\nelse:\n    encoder = keras.models.load_model(model_directory)\n    decoder = keras.models.load_model(model_directory)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-12T03:05:49.217075Z","iopub.execute_input":"2022-04-12T03:05:49.217997Z","iopub.status.idle":"2022-04-12T03:05:49.250578Z","shell.execute_reply.started":"2022-04-12T03:05:49.217951Z","shell.execute_reply":"2022-04-12T03:05:49.249015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(loss_plot)\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Loss Plot')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-10T23:56:34.992163Z","iopub.execute_input":"2022-04-10T23:56:34.992487Z","iopub.status.idle":"2022-04-10T23:56:35.127259Z","shell.execute_reply.started":"2022-04-10T23:56:34.992457Z","shell.execute_reply":"2022-04-10T23:56:35.126558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    os.makedirs('./images/')\nexcept:\n    pass","metadata":{"execution":{"iopub.status.busy":"2022-04-10T23:22:31.445577Z","iopub.execute_input":"2022-04-10T23:22:31.445954Z","iopub.status.idle":"2022-04-10T23:22:31.450449Z","shell.execute_reply.started":"2022-04-10T23:22:31.445915Z","shell.execute_reply":"2022-04-10T23:22:31.449515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate(image):\n    attention_plot = np.zeros((max_length, attention_features_shape))\n\n    hidden = decoder.reset_state(batch_size=1)\n\n    temp_input = tf.expand_dims(load_image(image)[0], 0)\n    img_tensor_val = image_features_extract_model(temp_input)\n    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0],-1, img_tensor_val.shape[3]))\n\n    features = encoder(img_tensor_val)\n\n    dec_input = tf.expand_dims([word_to_index('<start>')], 0)\n    result = []\n    result.append('<start>')\n\n    for i in range(max_length):\n        \n        predictions, hidden, attention_weights = decoder(dec_input,features,hidden)\n\n        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n\n        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n        #print(predicted_id)\n        predicted_word = tf.compat.as_text(index_to_word(int(predicted_id)).numpy())\n        result.append(predicted_word)\n\n        if predicted_word == '<end>':\n            return result, attention_plot\n\n        dec_input = tf.expand_dims([predicted_id], 0)\n\n    attention_plot = attention_plot[:len(result), :]\n    return result, attention_plot","metadata":{"execution":{"iopub.status.busy":"2022-04-10T23:22:31.451826Z","iopub.execute_input":"2022-04-10T23:22:31.452445Z","iopub.status.idle":"2022-04-10T23:22:31.465198Z","shell.execute_reply.started":"2022-04-10T23:22:31.452401Z","shell.execute_reply":"2022-04-10T23:22:31.464462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_attention(image, result, attention_plot, display_attention=False):\n    temp_image = np.array(Image.open(image))\n    plt.imshow(temp_image)\n\n    fig = plt.figure(figsize=(10,10))\n\n    len_result = len(result)\n    \n    if display_attention:\n        for i in range(len_result):\n            temp_att = np.resize(attention_plot[i], (8, 8))\n            grid_size = max(int(np.ceil(len_result/2)), 2)\n            ax = fig.add_subplot(grid_size, grid_size, i+1)\n            ax.set_title(result[i])\n            img = ax.imshow(temp_image)\n            ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n\n        plt.tight_layout()\n    plt.show()\n    #plt.plot()\n","metadata":{"execution":{"iopub.status.busy":"2022-04-10T23:31:38.135004Z","iopub.execute_input":"2022-04-10T23:31:38.135345Z","iopub.status.idle":"2022-04-10T23:31:38.144428Z","shell.execute_reply.started":"2022-04-10T23:31:38.135313Z","shell.execute_reply":"2022-04-10T23:31:38.143565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rid = np.random.randint(0, len(img_name_val))\nimage = img_name_val[rid]\nreal_caption = ' '.join([tf.compat.as_text(index_to_word(i).numpy())\n                         for i in cap_val[rid] if i not in [0]])\nresult, attention_plot = evaluate(image)\n\nprint('Real Caption:', real_caption)\nprint('Prediction Caption:', ' '.join(result))\nplot_attention(image, result, attention_plot,True)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T23:31:40.598862Z","iopub.execute_input":"2022-04-10T23:31:40.599206Z","iopub.status.idle":"2022-04-10T23:31:42.5214Z","shell.execute_reply.started":"2022-04-10T23:31:40.599174Z","shell.execute_reply":"2022-04-10T23:31:42.520593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Checking the bleu score","metadata":{}},{"cell_type":"code","source":"from nltk.translate.bleu_score import sentence_bleu\n\nBLEU1 = []\nBLEU2 = []\nBLEU3 = []\nBLEU4 = []\n\ncounter = 0 \ndef convert_to_caption(real_caption):\n    return ' '.join([tf.compat.as_text(index_to_word(i).numpy())\n                         for i in real_caption if i not in [0]])\n\nfor i in range(0,len(img_name_val),5):\n    references = []\n    [references.append(convert_to_caption(j)[8:-6]) for j in cap_val[i:i+5]]\n    \n    #print(references)\n    counter = counter+1\n    result,_ = evaluate(img_name_val[i])\n    predicted_caption = ' '.join(result)\n    #print(predicted_caption)\n    predicted_caption = predicted_caption[8:-5]\n\n    #print(predicted_caption)\n    bleu = sentence_bleu(references, predicted_caption, weights=(1, 0, 0, 0))\n    if bleu > 0.7 and counter<=5:\n        print(predicted_caption)\n        plot_attention(img_name_val[i], result, attention_plot)\n        print(\"-----------------------\\n\")\n        \n    BLEU1.append(sentence_bleu(references, predicted_caption, weights=(1, 0, 0, 0)))\n    BLEU2.append(sentence_bleu(references, predicted_caption, weights=(0, 1, 0, 0)))\n    BLEU3.append(sentence_bleu(references, predicted_caption, weights=(0, 0, 1, 0)))\n    BLEU4.append(sentence_bleu(references, predicted_caption, weights=(0, 0, 0, 1)))","metadata":{"execution":{"iopub.status.busy":"2022-04-11T00:02:16.743591Z","iopub.execute_input":"2022-04-11T00:02:16.743981Z","iopub.status.idle":"2022-04-11T00:02:18.605275Z","shell.execute_reply.started":"2022-04-11T00:02:16.743946Z","shell.execute_reply":"2022-04-11T00:02:18.604416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"The Average unigram Blue score on the validation set is\", np.mean(np.array(BLEU1)))\nprint(\"The Average bigram Blue score on the validation set is\", np.mean(np.array(BLEU2)))\nprint(\"The Average trigram Blue score on the validation set is\", np.mean(np.array(BLEU3)))\nprint(\"The Average fourgram Blue score on the validation set is\", np.mean(np.array(BLEU4)))","metadata":{"execution":{"iopub.status.busy":"2022-04-10T23:31:47.460502Z","iopub.execute_input":"2022-04-10T23:31:47.460897Z","iopub.status.idle":"2022-04-10T23:31:47.470415Z","shell.execute_reply.started":"2022-04-10T23:31:47.460859Z","shell.execute_reply":"2022-04-10T23:31:47.469416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# os.mkdir('results/')\n# np.save('results/',np.array(scores))\n# os.mkdir('models/')\n\n# encoder.save_weights('models/encoder_big_new')\n# decoder.save_weights('models/decoder_big_new')","metadata":{"execution":{"iopub.status.busy":"2022-04-10T19:43:43.237753Z","iopub.execute_input":"2022-04-10T19:43:43.238105Z","iopub.status.idle":"2022-04-10T19:43:43.243056Z","shell.execute_reply.started":"2022-04-10T19:43:43.238063Z","shell.execute_reply":"2022-04-10T19:43:43.242172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# rid = np.random.randint(0, len(img_name_val)//5)*5+1\n# for i in range(rid, rid+5,1):\n#     caption_decoded = ' '.join([tf.compat.as_text(index_to_word(j).numpy())\n#                          for j in cap_val[i] if j not in [0]])\n#     print(caption_decoded)\n    \n#     print(df.iloc[i]['caption'])\n#     print(\"-----------------\")\n    \n# # image = img_name_val[rid]\n# # temp_image = np.array(Image.open(image))\n# # plt.imshow(temp_image)\n# #for i in (rid, rid+5):\n    \n# #for i in range(0)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T14:58:10.577854Z","iopub.execute_input":"2022-04-10T14:58:10.578214Z","iopub.status.idle":"2022-04-10T14:58:10.582857Z","shell.execute_reply.started":"2022-04-10T14:58:10.578181Z","shell.execute_reply":"2022-04-10T14:58:10.581657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# rid = np.random.randint(0, len(img_name_val))\n# image = img_name_val[rid]\n# real_caption = ' '.join([tf.compat.as_text(index_to_word(i).numpy())\n#                          for i in cap_val[rid] if i not in [0]])\n# result, attention_plot = evaluate(image)\n# prediction_caption =  ' '.join(result)\n# print(\"the Blue score for the image is \", sentence_bleu(prediction_caption, real_caption))\n# print('Real Caption:', real_caption)\n# print('Prediction Caption:', ' '.join(result))\n# plot_attention(image, result, attention_plot)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T14:58:10.731483Z","iopub.execute_input":"2022-04-10T14:58:10.73189Z","iopub.status.idle":"2022-04-10T14:58:10.736358Z","shell.execute_reply.started":"2022-04-10T14:58:10.731855Z","shell.execute_reply":"2022-04-10T14:58:10.735338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from nltk.translate.bleu_score import sentence_bleu\n# index_word = dict([(index,word) for word, index in tokenizer.word_index.items()])\n\n\n# nkeep = 5\n# pred_good, pred_bad, bleus = [], [], [] \n# count = 0 \n# for jpgfnm, image_feature, tokenized_text in zip(fnm_test,di_test,dt_test):\n#     count += 1\n#     if count % 200 == 0:\n#         print(\"  {:4.2f}% is done..\".format(100*count/float(len(fnm_test))))\n    \n#     caption_true = [ index_word[i] for i in tokenized_text ]     \n#     caption_true = caption_true[1:-1] ## remove startreg, and endreg\n#     ## captions\n#     caption = predict_caption(image_feature.reshape(1,len(image_feature)))\n#     caption = caption.split()\n#     caption = caption[1:-1]## remove startreg, and endreg\n    \n#     bleu = sentence_bleu([caption_true],caption)\n#     bleus.append(bleu)\n#     if bleu > 0.7 and len(pred_good) < nkeep:\n#         pred_good.append((bleu,jpgfnm,caption_true,caption))\n#     elif bleu < 0.3 and len(pred_bad) < nkeep:\n#         pred_bad.append((bleu,jpgfnm,caption_true,caption))\n","metadata":{"execution":{"iopub.status.busy":"2022-04-10T14:58:10.894838Z","iopub.execute_input":"2022-04-10T14:58:10.895188Z","iopub.status.idle":"2022-04-10T14:58:10.899579Z","shell.execute_reply.started":"2022-04-10T14:58:10.895158Z","shell.execute_reply":"2022-04-10T14:58:10.898614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-10T14:50:20.425243Z","iopub.status.idle":"2022-04-10T14:50:20.426052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}