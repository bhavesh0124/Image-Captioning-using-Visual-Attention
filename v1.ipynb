{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-03T23:54:07.916374Z",
     "iopub.status.busy": "2022-04-03T23:54:07.916074Z",
     "iopub.status.idle": "2022-04-03T23:54:07.92188Z",
     "shell.execute_reply": "2022-04-03T23:54:07.921129Z",
     "shell.execute_reply.started": "2022-04-03T23:54:07.916323Z"
    }
   },
   "outputs": [],
   "source": [
    "#! pip install --ignore-installed  --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-05T06:19:06.815500Z",
     "iopub.status.busy": "2022-04-05T06:19:06.815217Z",
     "iopub.status.idle": "2022-04-05T06:19:11.377100Z",
     "shell.execute_reply": "2022-04-05T06:19:11.376268Z",
     "shell.execute_reply.started": "2022-04-05T06:19:06.815473Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tensorflow import keras\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from PIL import Image\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Resizing\n",
    "image_path = '../input/flickr8k/Images/'\n",
    "df = pd.read_csv('../input/flickr8k/captions.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-05T06:19:11.381155Z",
     "iopub.status.busy": "2022-04-05T06:19:11.380902Z",
     "iopub.status.idle": "2022-04-05T06:19:14.788404Z",
     "shell.execute_reply": "2022-04-05T06:19:14.787450Z",
     "shell.execute_reply.started": "2022-04-05T06:19:11.381129Z"
    }
   },
   "outputs": [],
   "source": [
    "image_path = '../input/flickr8k/Images/'\n",
    "image_path_to_caption = collections.defaultdict(list)\n",
    "\n",
    "captions = df['caption']\n",
    "train_captions = []\n",
    "img_name_vector = []\n",
    "\n",
    "for index,row in tqdm(df.iterrows()):\n",
    "    caption = f\"<start> {row['caption']} <end>\"\n",
    "    image_path_new  = image_path + row['image']\n",
    "    image_path_to_caption[image_path_new].append(caption)\n",
    "print(len(image_path_to_caption.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-05T06:19:14.789878Z",
     "iopub.status.busy": "2022-04-05T06:19:14.789546Z",
     "iopub.status.idle": "2022-04-05T06:19:14.796791Z",
     "shell.execute_reply": "2022-04-05T06:19:14.795825Z",
     "shell.execute_reply.started": "2022-04-05T06:19:14.789842Z"
    }
   },
   "outputs": [],
   "source": [
    "##change this to full dataset when training\n",
    "\n",
    "train_image_paths = list(image_path_to_caption.keys())\n",
    "train_image_paths = train_image_paths[:1000]\n",
    "\n",
    "random.shuffle(train_image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-05T06:19:14.798385Z",
     "iopub.status.busy": "2022-04-05T06:19:14.798036Z",
     "iopub.status.idle": "2022-04-05T06:19:14.807306Z",
     "shell.execute_reply": "2022-04-05T06:19:14.806604Z",
     "shell.execute_reply.started": "2022-04-05T06:19:14.798324Z"
    }
   },
   "outputs": [],
   "source": [
    "train_captions = []\n",
    "img_name_vector = []\n",
    "\n",
    "for path in train_image_paths:\n",
    "      caption_list = image_path_to_caption[path]\n",
    "      train_captions.extend(caption_list)\n",
    "      img_name_vector.extend([path] * len(caption_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change the above code by simply, converting the dataframe series to list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image_name_vector and train_captions are main lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-05T06:19:14.810365Z",
     "iopub.status.busy": "2022-04-05T06:19:14.809628Z",
     "iopub.status.idle": "2022-04-05T06:19:14.817051Z",
     "shell.execute_reply": "2022-04-05T06:19:14.816210Z",
     "shell.execute_reply.started": "2022-04-05T06:19:14.810183Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.io.decode_jpeg(img, channels=3)\n",
    "    img = Resizing(299, 299)(img)\n",
    "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "    return img, image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-04T00:12:03.39586Z",
     "iopub.status.busy": "2022-04-04T00:12:03.395549Z",
     "iopub.status.idle": "2022-04-04T00:12:08.700694Z",
     "shell.execute_reply": "2022-04-04T00:12:08.699801Z",
     "shell.execute_reply.started": "2022-04-04T00:12:03.395831Z"
    }
   },
   "outputs": [],
   "source": [
    "image_model = tf.keras.applications.InceptionV3(include_top=False,weights='imagenet')\n",
    "new_input = image_model.input\n",
    "hidden_layer = image_model.layers[-1].output\n",
    "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-04T00:12:08.702766Z",
     "iopub.status.busy": "2022-04-04T00:12:08.702419Z",
     "iopub.status.idle": "2022-04-04T00:12:18.142505Z",
     "shell.execute_reply": "2022-04-04T00:12:18.141522Z",
     "shell.execute_reply.started": "2022-04-04T00:12:08.702736Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get unique images ##extracting and saving the features\n",
    "encode_train = sorted(set(img_name_vector))\n",
    "\n",
    "# Feel free to change batch_size according to your system configuration\n",
    "image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n",
    "image_dataset = image_dataset.map(\n",
    "  load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(32)\n",
    "\n",
    "for img, path in image_dataset:\n",
    "  batch_features = image_features_extract_model(img)\n",
    "  batch_features = tf.reshape(batch_features,\n",
    "                              (batch_features.shape[0], -1, batch_features.shape[3]))\n",
    "  \n",
    "  for bf, p in zip(batch_features, path):\n",
    "    path_of_feature = p.numpy().decode(\"utf-8\")\n",
    "    path_of_feature = path_of_feature[25:]\n",
    "    np.save(path_of_feature, bf.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-04T00:12:18.144478Z",
     "iopub.status.busy": "2022-04-04T00:12:18.144114Z",
     "iopub.status.idle": "2022-04-04T00:12:21.371936Z",
     "shell.execute_reply": "2022-04-04T00:12:21.37115Z",
     "shell.execute_reply.started": "2022-04-04T00:12:18.144438Z"
    }
   },
   "outputs": [],
   "source": [
    "caption_dataset = tf.data.Dataset.from_tensor_slices(train_captions)\n",
    "\n",
    "# We will override the default standardization of TextVectorization to preserve\n",
    "# \"<>\" characters, so we preserve the tokens for the <start> and <end>.\n",
    "def standardize(inputs):\n",
    "  inputs = tf.strings.lower(inputs)\n",
    "  return tf.strings.regex_replace(inputs,\n",
    "                                  r\"!\\\"#$%&\\(\\)\\*\\+.,-/:;=?@\\[\\\\\\]^_`{|}~\", \"\")\n",
    "\n",
    "max_length = 50\n",
    "vocabulary_size = 1000\n",
    "tokenizer = tf.keras.layers.experimental.preprocessing.TextVectorization(max_tokens =vocabulary_size,\n",
    "                                                                         standardize=standardize,\n",
    "                                                                        output_sequence_length=50)\n",
    "\n",
    "tokenizer.adapt(caption_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-04T00:12:21.373835Z",
     "iopub.status.busy": "2022-04-04T00:12:21.373516Z",
     "iopub.status.idle": "2022-04-04T00:12:21.459867Z",
     "shell.execute_reply": "2022-04-04T00:12:21.459133Z",
     "shell.execute_reply.started": "2022-04-04T00:12:21.3738Z"
    }
   },
   "outputs": [],
   "source": [
    "def vectorize_text(text):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    return tokenizer(text)\n",
    "\n",
    "cap_vector = caption_dataset.map(vectorize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-04T00:12:21.463491Z",
     "iopub.status.busy": "2022-04-04T00:12:21.463055Z",
     "iopub.status.idle": "2022-04-04T00:12:21.548762Z",
     "shell.execute_reply": "2022-04-04T00:12:21.548093Z",
     "shell.execute_reply.started": "2022-04-04T00:12:21.463458Z"
    }
   },
   "outputs": [],
   "source": [
    "word_to_index = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "    mask_token=\"\",\n",
    "    vocabulary=tokenizer.get_vocabulary())\n",
    "\n",
    "index_to_word = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "    mask_token=\"\",\n",
    "    vocabulary=tokenizer.get_vocabulary(),\n",
    "    invert=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-04T00:12:21.552373Z",
     "iopub.status.busy": "2022-04-04T00:12:21.552092Z",
     "iopub.status.idle": "2022-04-04T00:12:24.145834Z",
     "shell.execute_reply": "2022-04-04T00:12:24.144999Z",
     "shell.execute_reply.started": "2022-04-04T00:12:21.55233Z"
    }
   },
   "outputs": [],
   "source": [
    "img_to_cap_vector = collections.defaultdict(list)\n",
    "for img, cap in zip(img_name_vector, cap_vector):\n",
    "  cap = tf.squeeze(cap)\n",
    "  img_to_cap_vector[img].append(cap)\n",
    "\n",
    "# Create training and validation sets using an 80-20 split randomly.\n",
    "img_keys = list(img_to_cap_vector.keys())\n",
    "random.shuffle(img_keys)\n",
    "\n",
    "slice_index = int(len(img_keys)*0.8)\n",
    "img_name_train_keys, img_name_val_keys = img_keys[:slice_index], img_keys[slice_index:]\n",
    "\n",
    "img_name_train = []\n",
    "cap_train = []\n",
    "for imgt in img_name_train_keys:\n",
    "  capt_len = len(img_to_cap_vector[imgt])\n",
    "  img_name_train.extend([imgt] * capt_len)\n",
    "  cap_train.extend(img_to_cap_vector[imgt])\n",
    "\n",
    "img_name_val = []\n",
    "cap_val = []\n",
    "for imgv in img_name_val_keys:\n",
    "  capv_len = len(img_to_cap_vector[imgv])\n",
    "  img_name_val.extend([imgv] * capv_len)\n",
    "  cap_val.extend(img_to_cap_vector[imgv])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-04T00:12:24.147362Z",
     "iopub.status.busy": "2022-04-04T00:12:24.147005Z",
     "iopub.status.idle": "2022-04-04T00:12:24.154863Z",
     "shell.execute_reply": "2022-04-04T00:12:24.153792Z",
     "shell.execute_reply.started": "2022-04-04T00:12:24.147312Z"
    }
   },
   "outputs": [],
   "source": [
    "len(img_name_train), len(cap_train), len(img_name_val), len(cap_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-04T00:12:32.34766Z",
     "iopub.status.busy": "2022-04-04T00:12:32.347259Z",
     "iopub.status.idle": "2022-04-04T00:12:32.353063Z",
     "shell.execute_reply": "2022-04-04T00:12:32.351794Z",
     "shell.execute_reply.started": "2022-04-04T00:12:32.347629Z"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 1000\n",
    "embedding_dim = 256\n",
    "units = 512\n",
    "num_steps = len(img_name_train) // BATCH_SIZE\n",
    "# Shape of the vector extracted from InceptionV3 is (64, 2048)\n",
    "# These two variables represent that vector shape\n",
    "features_shape = 2048\n",
    "attention_features_shape = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-04T00:12:33.008765Z",
     "iopub.status.busy": "2022-04-04T00:12:33.008442Z",
     "iopub.status.idle": "2022-04-04T00:12:33.014278Z",
     "shell.execute_reply": "2022-04-04T00:12:33.013411Z",
     "shell.execute_reply.started": "2022-04-04T00:12:33.008737Z"
    }
   },
   "outputs": [],
   "source": [
    "def map_func(img_name, cap):\n",
    "  img_name = img_name.decode('utf-8')[25:]\n",
    "  img_tensor = np.load('./'+ img_name  +'.npy')\n",
    "  return img_tensor, cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-04T00:12:33.590297Z",
     "iopub.status.busy": "2022-04-04T00:12:33.589992Z",
     "iopub.status.idle": "2022-04-04T00:12:33.738136Z",
     "shell.execute_reply": "2022-04-04T00:12:33.737283Z",
     "shell.execute_reply.started": "2022-04-04T00:12:33.590267Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\n",
    "\n",
    "# Use map to load the numpy files in parallel\n",
    "dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n",
    "          map_func, [item1, item2], [tf.float32, tf.int64]),\n",
    "          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Shuffle and batch\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-04T00:12:34.476826Z",
     "iopub.status.busy": "2022-04-04T00:12:34.476508Z",
     "iopub.status.idle": "2022-04-04T00:12:34.486625Z",
     "shell.execute_reply": "2022-04-04T00:12:34.485436Z",
     "shell.execute_reply.started": "2022-04-04T00:12:34.476797Z"
    }
   },
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "  def __init__(self, units):\n",
    "    super(BahdanauAttention, self).__init__()\n",
    "    self.W1 = tf.keras.layers.Dense(units)\n",
    "    self.W2 = tf.keras.layers.Dense(units)\n",
    "    self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "  def call(self, features, hidden):\n",
    "    # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n",
    "\n",
    "    # hidden shape == (batch_size, hidden_size)\n",
    "    # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
    "    hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "    # attention_hidden_layer shape == (batch_size, 64, units)\n",
    "    attention_hidden_layer = (tf.nn.tanh(self.W1(features) +\n",
    "                                         self.W2(hidden_with_time_axis)))\n",
    "\n",
    "    # score shape == (batch_size, 64, 1)\n",
    "    # This gives you an unnormalized score for each image feature.\n",
    "    score = self.V(attention_hidden_layer)\n",
    "\n",
    "    # attention_weights shape == (batch_size, 64, 1)\n",
    "    attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "    # context_vector shape after sum == (batch_size, hidden_size)\n",
    "    context_vector = attention_weights * features\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "    return context_vector, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-04T00:12:35.00024Z",
     "iopub.status.busy": "2022-04-04T00:12:34.999929Z",
     "iopub.status.idle": "2022-04-04T00:12:35.006616Z",
     "shell.execute_reply": "2022-04-04T00:12:35.005523Z",
     "shell.execute_reply.started": "2022-04-04T00:12:35.00021Z"
    }
   },
   "outputs": [],
   "source": [
    "class CNN_Encoder(tf.keras.Model):\n",
    "    # Since you have already extracted the features and dumped it\n",
    "    # This encoder passes those features through a Fully connected layer\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        # shape after fc == (batch_size, 64, embedding_dim)\n",
    "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-04T00:14:01.640414Z",
     "iopub.status.busy": "2022-04-04T00:14:01.640071Z",
     "iopub.status.idle": "2022-04-04T00:14:01.652593Z",
     "shell.execute_reply": "2022-04-04T00:14:01.651537Z",
     "shell.execute_reply.started": "2022-04-04T00:14:01.640381Z"
    }
   },
   "outputs": [],
   "source": [
    "class RNN_Decoder(tf.keras.Model):\n",
    "  def __init__(self, embedding_dim, units, vocab_size):\n",
    "    super(RNN_Decoder, self).__init__()\n",
    "    self.units = units\n",
    "\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "    self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "  def call(self, x, features, hidden):\n",
    "    # defining attention as a separate model\n",
    "    context_vector, attention_weights = self.attention(features, hidden)\n",
    "\n",
    "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "    x = self.embedding(x)\n",
    "\n",
    "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "    # passing the concatenated vector to the GRU\n",
    "    output, state = self.gru(x)\n",
    "\n",
    "    # shape == (batch_size, max_length, hidden_size)\n",
    "    x = self.fc1(output)\n",
    "\n",
    "    # x shape == (batch_size * max_length, hidden_size)\n",
    "    x = tf.reshape(x, (-1, x.shape[2]))\n",
    "\n",
    "    # output shape == (batch_size * max_length, vocab)\n",
    "    x = self.fc2(x)\n",
    "\n",
    "    return x, state, attention_weights\n",
    "\n",
    "\n",
    "  def reset_state(self, batch_size):\n",
    "    return tf.zeros((batch_size, self.units))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-04T00:14:03.463179Z",
     "iopub.status.busy": "2022-04-04T00:14:03.462854Z",
     "iopub.status.idle": "2022-04-04T00:14:03.48986Z",
     "shell.execute_reply": "2022-04-04T00:14:03.489183Z",
     "shell.execute_reply.started": "2022-04-04T00:14:03.463149Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder = CNN_Encoder(embedding_dim)\n",
    "decoder = RNN_Decoder(embedding_dim, units, len(tokenizer.get_vocabulary()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-04T00:14:05.376597Z",
     "iopub.status.busy": "2022-04-04T00:14:05.376142Z",
     "iopub.status.idle": "2022-04-04T00:14:05.390061Z",
     "shell.execute_reply": "2022-04-04T00:14:05.389115Z",
     "shell.execute_reply.started": "2022-04-04T00:14:05.376557Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam() ##change maybe different optimizer\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-04T00:14:05.756726Z",
     "iopub.status.busy": "2022-04-04T00:14:05.756424Z",
     "iopub.status.idle": "2022-04-04T00:14:05.764531Z",
     "shell.execute_reply": "2022-04-04T00:14:05.76375Z",
     "shell.execute_reply.started": "2022-04-04T00:14:05.756697Z"
    }
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/train\"\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer=optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-04T00:14:07.266238Z",
     "iopub.status.busy": "2022-04-04T00:14:07.265915Z",
     "iopub.status.idle": "2022-04-04T00:14:07.271322Z",
     "shell.execute_reply": "2022-04-04T00:14:07.270449Z",
     "shell.execute_reply.started": "2022-04-04T00:14:07.266207Z"
    }
   },
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "  start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
    "  # restoring the latest checkpoint in checkpoint_path\n",
    "  ckpt.restore(ckpt_manager.latest_checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-04T00:14:08.809498Z",
     "iopub.status.busy": "2022-04-04T00:14:08.809075Z",
     "iopub.status.idle": "2022-04-04T00:14:08.813434Z",
     "shell.execute_reply": "2022-04-04T00:14:08.81247Z",
     "shell.execute_reply.started": "2022-04-04T00:14:08.809457Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_plot = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-04T00:14:59.665864Z",
     "iopub.status.busy": "2022-04-04T00:14:59.665543Z",
     "iopub.status.idle": "2022-04-04T00:14:59.679118Z",
     "shell.execute_reply": "2022-04-04T00:14:59.678177Z",
     "shell.execute_reply.started": "2022-04-04T00:14:59.665834Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_step(img_tensor, target):\n",
    "  loss = 0\n",
    "\n",
    "  # initializing the hidden state for each batch\n",
    "  # because the captions are not related from image to image\n",
    "  hidden = decoder.reset_state(batch_size=target.shape[0])\n",
    "\n",
    "  dec_input = tf.expand_dims([word_to_index('<start>')] * target.shape[0], 1)\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "      features = encoder(img_tensor)\n",
    "      #print(features.shape)\n",
    "      #print(target)\n",
    "      for i in range(1, target.shape[1]):\n",
    "          # passing the features through the decoder\n",
    "          predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "          #print(predictions,hidden)\n",
    "\n",
    "          loss += loss_function(target[:, i], predictions)\n",
    "\n",
    "          # using teacher forcing\n",
    "          dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "  total_loss = (loss / int(target.shape[1]))\n",
    "\n",
    "  trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "  gradients = tape.gradient(loss, trainable_variables)\n",
    "\n",
    "  optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "  return loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-04T00:15:05.171984Z",
     "iopub.status.busy": "2022-04-04T00:15:05.171662Z",
     "iopub.status.idle": "2022-04-04T00:29:22.005839Z",
     "shell.execute_reply": "2022-04-04T00:29:22.004834Z",
     "shell.execute_reply.started": "2022-04-04T00:15:05.171954Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "EPOCHS = 20\n",
    "\n",
    "for epoch in tqdm(range(start_epoch, EPOCHS)):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (img_tensor, target)) in enumerate(dataset):\n",
    "        batch_loss, t_loss = train_step(img_tensor, target)\n",
    "        total_loss += t_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            average_batch_loss = batch_loss.numpy()/int(target.shape[1])\n",
    "            print(f'Epoch {epoch+1} Batch {batch} Loss {average_batch_loss:.4f}')\n",
    "    # storing the epoch end loss value to plot later\n",
    "    loss_plot.append(total_loss / num_steps)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "      ckpt_manager.save()\n",
    "\n",
    "    print(f'Epoch {epoch+1} Loss {total_loss/num_steps:.6f}')\n",
    "    print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-04T00:29:22.008701Z",
     "iopub.status.busy": "2022-04-04T00:29:22.008071Z",
     "iopub.status.idle": "2022-04-04T00:29:22.179109Z",
     "shell.execute_reply": "2022-04-04T00:29:22.178359Z",
     "shell.execute_reply.started": "2022-04-04T00:29:22.00866Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(loss_plot)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Plot')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-03T23:04:45.076346Z",
     "iopub.status.busy": "2022-04-03T23:04:45.075915Z",
     "iopub.status.idle": "2022-04-03T23:04:45.080742Z",
     "shell.execute_reply": "2022-04-03T23:04:45.079329Z",
     "shell.execute_reply.started": "2022-04-03T23:04:45.076312Z"
    }
   },
   "outputs": [],
   "source": [
    "##saving the encoders and decoders model\n",
    "# os.makedirs('./models/')\n",
    "# encoder.save('encoder')\n",
    "# decoder.save('decoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-04T00:29:22.180689Z",
     "iopub.status.busy": "2022-04-04T00:29:22.180368Z",
     "iopub.status.idle": "2022-04-04T00:29:22.192553Z",
     "shell.execute_reply": "2022-04-04T00:29:22.191619Z",
     "shell.execute_reply.started": "2022-04-04T00:29:22.180662Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(image):\n",
    "    attention_plot = np.zeros((max_length, attention_features_shape))\n",
    "\n",
    "    hidden = decoder.reset_state(batch_size=1)\n",
    "\n",
    "    temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
    "    img_tensor_val = image_features_extract_model(temp_input)\n",
    "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0],\n",
    "                                                 -1,\n",
    "                                                 img_tensor_val.shape[3]))\n",
    "\n",
    "    features = encoder(img_tensor_val)\n",
    "\n",
    "    dec_input = tf.expand_dims([word_to_index('<start>')], 0)\n",
    "    result = []\n",
    "\n",
    "    for i in range(max_length):\n",
    "        predictions, hidden, attention_weights = decoder(dec_input,\n",
    "                                                         features,\n",
    "                                                         hidden)\n",
    "\n",
    "        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
    "\n",
    "        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
    "        #print(predicted_id)\n",
    "        predicted_word = tf.compat.as_text(index_to_word(int(predicted_id)).numpy())\n",
    "        result.append(predicted_word)\n",
    "\n",
    "        if predicted_word == '<end>':\n",
    "            return result, attention_plot\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    attention_plot = attention_plot[:len(result), :]\n",
    "    return result, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-04T00:29:22.194487Z",
     "iopub.status.busy": "2022-04-04T00:29:22.193959Z",
     "iopub.status.idle": "2022-04-04T00:29:22.207336Z",
     "shell.execute_reply": "2022-04-04T00:29:22.206577Z",
     "shell.execute_reply.started": "2022-04-04T00:29:22.194451Z"
    }
   },
   "outputs": [],
   "source": [
    "os.makedirs('./images/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-04T00:29:22.210549Z",
     "iopub.status.busy": "2022-04-04T00:29:22.210178Z",
     "iopub.status.idle": "2022-04-04T00:29:22.220714Z",
     "shell.execute_reply": "2022-04-04T00:29:22.219685Z",
     "shell.execute_reply.started": "2022-04-04T00:29:22.210502Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_attention(image, result, attention_plot):\n",
    "    temp_image = np.array(Image.open(image))\n",
    "    plt.imshow(temp_image)\n",
    "\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "\n",
    "    len_result = len(result)\n",
    "    for i in range(len_result):\n",
    "        temp_att = np.resize(attention_plot[i], (8, 8))\n",
    "        grid_size = max(int(np.ceil(len_result/2)), 2)\n",
    "        ax = fig.add_subplot(grid_size, grid_size, i+1)\n",
    "        ax.set_title(result[i])\n",
    "        img = ax.imshow(temp_image)\n",
    "        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    #plt.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-04T00:35:01.206189Z",
     "iopub.status.busy": "2022-04-04T00:35:01.20593Z",
     "iopub.status.idle": "2022-04-04T00:35:03.537272Z",
     "shell.execute_reply": "2022-04-04T00:35:03.536409Z",
     "shell.execute_reply.started": "2022-04-04T00:35:01.206163Z"
    }
   },
   "outputs": [],
   "source": [
    "rid = np.random.randint(0, len(img_name_val))\n",
    "image = img_name_val[rid]\n",
    "real_caption = ' '.join([tf.compat.as_text(index_to_word(i).numpy())\n",
    "                         for i in cap_val[rid] if i not in [0]])\n",
    "result, attention_plot = evaluate(image)\n",
    "\n",
    "print('Real Caption:', real_caption)\n",
    "print('Prediction Caption:', ' '.join(result))\n",
    "plot_attention(image, result, attention_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing with our own image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-04T00:59:03.247589Z",
     "iopub.status.busy": "2022-04-04T00:59:03.247227Z",
     "iopub.status.idle": "2022-04-04T00:59:04.692088Z",
     "shell.execute_reply": "2022-04-04T00:59:04.691261Z",
     "shell.execute_reply.started": "2022-04-04T00:59:03.247558Z"
    }
   },
   "outputs": [],
   "source": [
    "test_image_path = '../input/imagetestgeegees/test.jpg'\n",
    "result, attention_plot = evaluate(test_image_path)\n",
    "print('Prediction Caption:', ' '.join(result))\n",
    "plot_attention(test_image_path, result, attention_plot)\n",
    "# opening the image\n",
    "#Image.open(test_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
